{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab510d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется Apple Silicon (MPS)\n",
      "Device: mps\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Используется Apple Silicon (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Используется CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Используется CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c7ff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Данные загружены:\n",
      "   Train: 52,854 пар\n",
      "   Val:   6,600 пар\n",
      "   Test:  6,607 пар\n",
      "\n",
      "=== ПРИМЕРЫ ДАННЫХ ===\n",
      "\n",
      "Пример 1:\n",
      "  INPUT:  Поехали два депутата городской думы на рыбалку. Порыбачили, сели выпить, покушат...\n",
      "  OUTPUT: Нет, Миш. Совпадения, не совпадения, а новый морг я уже успел пообещать…\n",
      "\n",
      "Пример 2:\n",
      "  INPUT:  В баре трое охотников рассказывают друг другу о своих успехах на сафари. Первый:...\n",
      "  OUTPUT: А на меня помчался буйвол с наставленными рогами, ну, я в него - бух!\n",
      "\n",
      "Пример 3:\n",
      "  INPUT:  Утром просыпаются муж и жена. Жена толкает мужа локтем в бок: - Дорогой, а ведь ...\n",
      "  OUTPUT: Да? Ну и что?\n"
     ]
    }
   ],
   "source": [
    "# === ЗАГРУЗКА ПОДГОТОВЛЕННЫХ ДАННЫХ ===\n",
    "\n",
    "# Загружаем CSV файлы из preprocessing\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "val_df = pd.read_csv('val_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(f\"\\nДанные загружены:\")\n",
    "print(f\"   Train: {len(train_df):,} пар\")\n",
    "print(f\"   Val:   {len(val_df):,} пар\")\n",
    "print(f\"   Test:  {len(test_df):,} пар\")\n",
    "\n",
    "# Примеры данных\n",
    "print(\"\\n=== ПРИМЕРЫ ДАННЫХ ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nПример {i+1}:\")\n",
    "    print(f\"  INPUT:  {train_df.iloc[i]['input'][:80]}...\")\n",
    "    print(f\"  OUTPUT: {train_df.iloc[i]['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0a7861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизатор загружен\n",
      "   Vocab size: 50,257\n",
      "   PAD token: '<pad>'\n",
      "\n",
      "Загрузка модели: sberbank-ai/rugpt3small_based_on_gpt2\n",
      "Модель загружена\n",
      "   Параметров: 125,226,240\n",
      "   Размер: 477.7 MB\n"
     ]
    }
   ],
   "source": [
    "# === ЗАГРУЗКА ТОКЕНИЗАТОРА И МОДЕЛИ ===\n",
    "MODEL_NAME = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Настройка токенизатора\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Токенизатор загружен\")\n",
    "print(f\"   Vocab size: {len(tokenizer):,}\")\n",
    "print(f\"   PAD token: '{tokenizer.pad_token}'\")\n",
    "\n",
    "print(f\"\\nЗагрузка модели: {MODEL_NAME}\")\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Настройка модели\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(\"Модель загружена\")\n",
    "print(f\"   Параметров: {model.num_parameters():,}\")\n",
    "print(f\"   Размер: {model.num_parameters() * 4 / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfddfca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Конвертация в HuggingFace Dataset...\n",
      "\n",
      "Токенизация данных...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd301ef672044f79a5e9b33d01e9eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Токенизация train:   0%|          | 0/52854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99a11e98ec9427c94da9a1b50b5655f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Токенизация val:   0%|          | 0/6600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Данные подготовлены:\n",
      "   Train dataset: 52854 примеров\n",
      "   Val dataset: 6600 примеров\n",
      "\n",
      "=== ПРИМЕР ТОКЕНИЗИРОВАННЫХ ДАННЫХ ===\n",
      "Input IDs shape: 160\n",
      "Decoded text: <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>Поехали два депутата городской думы на рыбалку. Порыбачили, сели выпить, покушать. - Петрович! Ты чего после аварии тако...\n"
     ]
    }
   ],
   "source": [
    "# === ПОДГОТОВКА ДАННЫХ ДЛЯ ОБУЧЕНИЯ ===\n",
    "\n",
    "# Гиперпараметры токенизации (из preprocessing)\n",
    "MAX_INPUT_LENGTH = 128\n",
    "MAX_OUTPUT_LENGTH = 32\n",
    "\n",
    "def format_prompt(input_text, output_text=None):\n",
    "    \"\"\"\n",
    "    Форматирует промпт для диалоговой модели.\n",
    "    Формат: <контекст> [SEP] <ответ> [EOS]\n",
    "    \"\"\"\n",
    "    if output_text:\n",
    "        # Для обучения: добавляем ответ\n",
    "        prompt = f\"{input_text}\\n{output_text}{tokenizer.eos_token}\"\n",
    "    else:\n",
    "        # Для инференса: только контекст\n",
    "        prompt = f\"{input_text}\\n\"\n",
    "    return prompt\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Токенизирует примеры для обучения.\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "        format_prompt(inp, out) \n",
    "        for inp, out in zip(examples['input'], examples['output'])\n",
    "    ]\n",
    "    \n",
    "    # Токенизируем\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH + MAX_OUTPUT_LENGTH,\n",
    "        padding='max_length',\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Для языковой модели labels = input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Конвертируем в Dataset\n",
    "print(\"\\nКонвертация в HuggingFace Dataset...\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "print(\"\\nТокенизация данных...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['input', 'output'],\n",
    "    desc=\"Токенизация train\"\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['input', 'output'],\n",
    "    desc=\"Токенизация val\"\n",
    ")\n",
    "\n",
    "print(f\"\\nДанные подготовлены:\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} примеров\")\n",
    "print(f\"   Val dataset: {len(val_dataset)} примеров\")\n",
    "\n",
    "# Пример токенизированных данных\n",
    "print(\"\\n=== ПРИМЕР ТОКЕНИЗИРОВАННЫХ ДАННЫХ ===\")\n",
    "example = train_dataset[0]\n",
    "print(f\"Input IDs shape: {len(example['input_ids'])}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(example['input_ids'])[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aab2dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметры обучения настроены:\n",
      "   Epochs: 3\n",
      "   Batch size: 8\n",
      "   Learning rate: 5e-05\n",
      "   Warmup steps: 500\n",
      "   Total training steps: ~19818\n"
     ]
    }
   ],
   "source": [
    "# === НАСТРОЙКА ОБУЧЕНИЯ ===\n",
    "\n",
    "# Создаем директорию для сохранения модели\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Основные параметры\n",
    "    output_dir=\"./models\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Параметры обучения\n",
    "    num_train_epochs=3,  # Немного эпох для baseline\n",
    "    per_device_train_batch_size=8,  # Небольшой batch для M4 Pro\n",
    "    per_device_eval_batch_size=8,\n",
    "    \n",
    "    # Оптимизация\n",
    "    learning_rate=5e-5,  # Стандартный LR для файнтюнинга\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # Логирование и сохранение\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,  # Храним только 2 последних чекпоинта\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    # Оптимизация для M4 Pro\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Параметры обучения настроены:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"   Total training steps: ~{len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "942a299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer создан\n",
      "   Training samples: 52854\n",
      "   Validation samples: 6600\n",
      "   Batch size: 8\n",
      "   Gradient accumulation steps: 1\n"
     ]
    }
   ],
   "source": [
    "# === СОЗДАНИЕ TRAINER ===\n",
    "# Data collator для языковой модели\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Создаем Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer создан\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6efacdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Старт обучения...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19821' max='19821' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19821/19821 2:28:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.362900</td>\n",
       "      <td>3.185733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.220700</td>\n",
       "      <td>3.058418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.149700</td>\n",
       "      <td>2.971968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.032100</td>\n",
       "      <td>2.888084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.006500</td>\n",
       "      <td>2.835291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.897000</td>\n",
       "      <td>2.768488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.889000</td>\n",
       "      <td>2.713115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.836100</td>\n",
       "      <td>2.671178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.748400</td>\n",
       "      <td>2.619606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.763800</td>\n",
       "      <td>2.586241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.658300</td>\n",
       "      <td>2.539321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.702600</td>\n",
       "      <td>2.490747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.628000</td>\n",
       "      <td>2.452744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.324200</td>\n",
       "      <td>2.428062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.384400</td>\n",
       "      <td>2.406282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.337700</td>\n",
       "      <td>2.374722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.338600</td>\n",
       "      <td>2.353128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.318700</td>\n",
       "      <td>2.312677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.229200</td>\n",
       "      <td>2.286885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.263100</td>\n",
       "      <td>2.261251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.244200</td>\n",
       "      <td>2.248987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.198400</td>\n",
       "      <td>2.220885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.199600</td>\n",
       "      <td>2.201154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.181600</td>\n",
       "      <td>2.182879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.165400</td>\n",
       "      <td>2.171522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.194300</td>\n",
       "      <td>2.144317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.981900</td>\n",
       "      <td>2.148074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>2.130728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.934200</td>\n",
       "      <td>2.118748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.944200</td>\n",
       "      <td>2.105084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.920500</td>\n",
       "      <td>2.094860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.883600</td>\n",
       "      <td>2.086087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.895200</td>\n",
       "      <td>2.077593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.906200</td>\n",
       "      <td>2.064415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.909300</td>\n",
       "      <td>2.053523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.909800</td>\n",
       "      <td>2.049715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.944500</td>\n",
       "      <td>2.041317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.877500</td>\n",
       "      <td>2.042466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.897300</td>\n",
       "      <td>2.039115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ОБУЧЕНИЕ ЗАВЕРШЕНО!\n",
      "================================================================================\n",
      "\n",
      "Финальные метрики:\n",
      "   Train Loss: 2.3803\n",
      "   Train Runtime: 8888.74 секунд\n",
      "   Train Samples/Second: 17.84\n",
      "Модель сохранена\n"
     ]
    }
   ],
   "source": [
    "# === ОБУЧЕНИЕ МОДЕЛИ ===\n",
    "print(\"\\nСтарт обучения...\")\n",
    "\n",
    "# Обучаем модель\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ОБУЧЕНИЕ ЗАВЕРШЕНО!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Выводим метрики обучения\n",
    "print(f\"\\nФинальные метрики:\")\n",
    "print(f\"   Train Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Train Runtime: {train_result.metrics['train_runtime']:.2f} секунд\")\n",
    "print(f\"   Train Samples/Second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Сохраняем финальную модель\n",
    "trainer.save_model(\"./models/final\")\n",
    "tokenizer.save_pretrained(\"./models/final\")\n",
    "\n",
    "print(\"Модель сохранена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2372f0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка модели на validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='825' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [825/825 01:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики на validation:\n",
      "   Eval Loss: 2.0497\n",
      "   Perplexity: 7.77\n",
      "   Eval Runtime: 71.84 секунд\n"
     ]
    }
   ],
   "source": [
    "# === EVALUATION НА ВАЛИДАЦИОННОМ СЕТЕ ===\n",
    "print(\"\\nОценка модели на validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\nМетрики на validation:\")\n",
    "print(f\"   Eval Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"   Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n",
    "print(f\"   Eval Runtime: {eval_results['eval_runtime']:.2f} секунд\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a811ebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ПРИМЕРЫ ГЕНЕРАЦИИ ===\n",
      "\n",
      "Пример 1:\n",
      "  Контекст: Женщина у психиатра: - Доктор, меня беспокоит мой сын. Он целыми днями лепит куличи из песка и делае...\n",
      "  Истинный ответ: Ничего страшного, это нормальное поведение ребенка.\n",
      "  Сгенерированный: Так, так. Сдайте ребенка психиатру. Он должен это все время делать на глазах у жены. И не стесняйтесь, пожалуйста, говорить ему\n",
      "\n",
      "Пример 2:\n",
      "  Контекст: Решил Байден посмотреть, как простой народ живет, загримировался, переоделся, пошел на ярмарку. Подх...\n",
      "  Истинный ответ: Здравствуйте, господин президент\n",
      "  Сгенерированный: Здравствуйте, я президент секретной службы, вот уже два дня у меня нет посетителей, у меня нет ни одного костюма, а вы просто за мной\n",
      "\n",
      "Пример 3:\n",
      "  Контекст: - Мама, а что мы будем праздновать в День отмены рабства? - Это не наш праздник, сынок....\n",
      "  Истинный ответ: Потому что мы белые?\n",
      "  Сгенерированный: А мой папа - не наш, он давно умер, и мы не отмечаем его. А мой дедушка - не наш, он тоже давно умер.\n",
      "\n",
      "Пример 4:\n",
      "  Контекст: - Вася, что мы имеем с курицы? - Яйца. - Вася, ну все-таки, подумай, что мы имеем с куриц? - Много я...\n",
      "  Истинный ответ: А если по подушке ударить, что будет?\n",
      "  Сгенерированный: А если по подушке ударить, что будет? Пыль\n",
      "\n",
      "Пример 5:\n",
      "  Контекст: Женщина приходит в синагогу и ищет раввина. Помощник сообщает ей, что раввин уехал в командировку, и...\n",
      "  Истинный ответ: Ну тогда вернется.\n",
      "  Сгенерированный: Вот видите, он оглянулся и снова оглянулся. А потом опять оглянулся и снова оглянулся и снова оглянулся. И снова повернулся.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === ТЕСТИРОВАНИЕ ГЕНЕРАЦИИ ===\n",
    "\n",
    "def generate_response(context, max_length=50, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Генерирует ответ на основе контекста.\n",
    "    \"\"\"\n",
    "    # Форматируем промпт\n",
    "    prompt = format_prompt(context)\n",
    "    \n",
    "    # Токенизируем\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Переносим на нужное устройство\n",
    "    if device.type != \"cpu\":\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        model.to(device)\n",
    "    \n",
    "    # Генерируем\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Декодируем\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Извлекаем только сгенерированную часть (после контекста)\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Тестируем на примерах из валидации\n",
    "print(\"\\n=== ПРИМЕРЫ ГЕНЕРАЦИИ ===\\n\")\n",
    "\n",
    "test_samples = val_df.sample(5, random_state=42)\n",
    "\n",
    "for idx, (i, row) in enumerate(test_samples.iterrows(), 1):\n",
    "    context = row['input']\n",
    "    true_response = row['output']\n",
    "    \n",
    "    generated = generate_response(context, max_length=30, temperature=0.7)\n",
    "    \n",
    "    print(f\"Пример {idx}:\")\n",
    "    print(f\"  Контекст: {context[:100]}...\")\n",
    "    print(f\"  Истинный ответ: {true_response}\")\n",
    "    print(f\"  Сгенерированный: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры контекстов для тестирования:\n",
      "\n",
      "1. - Почему программисты не любят природу?\n",
      "2. Встречаются два друга:\n",
      "- Как дела?\n",
      "3. Приходит мужик в аптеку и говорит:\n",
      "4. - Доктор, у меня проблемы с памятью.\n",
      "5. Разговор двух подруг:\n",
      "- Как твой новый парень?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Контекст: - Почему программисты не любят природу?\n",
      "Ответ модели: Понимаешь, когда ты делаешь что-то, то сразу видно, что это сделано программистом, а когда делаешь что-то, то сразу понимаешь, что это сделано кем-то другим,\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Контекст: Встречаются два друга:\n",
      "- Как дела?\n",
      "Ответ модели: Да, не спрашивай... У меня теперь есть машина... Тойота... ВАЗ... Мерседес... ВАЗ-2101... Да ну его!.. Иду вчера вечером в вечернюю школу, такая класс\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Контекст: Приходит мужик в аптеку и говорит:\n",
      "Ответ модели: Мне, пожалуйста, упаковку 22 с сиропом и 100мл. презервативов, а то у меня руки трясутся. Аптекарша: - Мужчина, а может, вам\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Контекст: - Доктор, у меня проблемы с памятью.\n",
      "Ответ модели: Ну, это легко. Вот вы, например, в своей жизни никогда не забывали поздравить с днем рождения. А вот если бы вы все-таки не дожили до дня рождения, вы бы могли\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Контекст: Разговор двух подруг:\n",
      "- Как твой новый парень?\n",
      "Ответ модели: Да все хорошо, он в сексе не новичок, но как только начинает приставать ко мне, сразу заявляет, что хочет трахнуть меня до смерти. А я ему говорю, что ему\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Примеры для тестирования\n",
    "test_contexts = [\n",
    "    \"- Почему программисты не любят природу?\",\n",
    "    \"Встречаются два друга:\\n- Как дела?\",\n",
    "    \"Приходит мужик в аптеку и говорит:\",\n",
    "    \"- Доктор, у меня проблемы с памятью.\",\n",
    "    \"Разговор двух подруг:\\n- Как твой новый парень?\",\n",
    "]\n",
    "\n",
    "print(\"Примеры контекстов для тестирования:\\n\")\n",
    "for i, ctx in enumerate(test_contexts, 1):\n",
    "    print(f\"{i}. {ctx}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "# Можно закомментировать для автоматического тестирования\n",
    "for ctx in test_contexts:\n",
    "    print(f\"\\nКонтекст: {ctx}\")\n",
    "    response = generate_response(ctx, max_length=40, temperature=0.8)\n",
    "    print(f\"Ответ модели: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5322aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "МОДЕЛЬ ОБУЧЕНА\n",
      "\n",
      "1. АРХИТЕКТУРА:\n",
      "   • Модель: sberbank-ai/rugpt3small_based_on_gpt2\n",
      "   • Параметров: 125,226,240\n",
      "   • Размер: 477.7 MB\n",
      "\n",
      "2. ПАРАМЕТРЫ ОБУЧЕНИЯ:\n",
      "   • Epochs: 3\n",
      "   • Batch size: 8\n",
      "   • Learning rate: 5e-05\n",
      "   • Training samples: 52,854\n",
      "\n",
      "3. РЕЗУЛЬТАТЫ:\n",
      "   • Final Train Loss: 2.3803\n",
      "   • Validation Loss: 2.0497\n",
      "   • Perplexity: 7.77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === ВЫВОДЫ ===\n",
    "\n",
    "summary = f\"\"\"\n",
    "МОДЕЛЬ ОБУЧЕНА\n",
    "\n",
    "1. АРХИТЕКТУРА:\n",
    "   • Модель: {MODEL_NAME}\n",
    "   • Параметров: {model.num_parameters():,}\n",
    "   • Размер: {model.num_parameters() * 4 / 1024 / 1024:.1f} MB\n",
    "\n",
    "2. ПАРАМЕТРЫ ОБУЧЕНИЯ:\n",
    "   • Epochs: {training_args.num_train_epochs}\n",
    "   • Batch size: {training_args.per_device_train_batch_size}\n",
    "   • Learning rate: {training_args.learning_rate}\n",
    "   • Training samples: {len(train_dataset):,}\n",
    "\n",
    "3. РЕЗУЛЬТАТЫ:\n",
    "   • Final Train Loss: {train_result.training_loss:.4f}\n",
    "   • Validation Loss: {eval_results['eval_loss']:.4f}\n",
    "   • Perplexity: {np.exp(eval_results['eval_loss']):.2f}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
